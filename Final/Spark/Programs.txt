Word count example:
---------------------
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SparkWordCount {
  def main(args: Array[String]) {
    // create Spark context with Spark configuration
    val sc = new SparkContext(new SparkConf().setAppName("Spark Count"))
	val textFile = sc.textFile("hdfs://...")
	val words = textFile.flatMap(line => line.split(" "))
	val groupedWords = words.map(word => (word, 1))
	val count = groupedWords.reduceByKey(_ + _)
	counts.saveAsTextFile("hdfs://...")
   }
}

spark 2.x example using DataSet and SparkSession :
-----------------------------------
import org.apache.spark.sql.SparkSession
object DataSetWordCount {
  def main(args: Array[String]) {

    val sparkSession = SparkSession.builder.master("local")
      .appName("example")
      .getOrCreate()

    import sparkSession.implicits._
    val data = sparkSession.read.text("src/main/resources/data.txt").as[String]
    val words = data.flatMap(line => line.split(" "))
	val groupedWords = words.map(word => (word, 1))
	val count = groupedWords.reduceByKey(_ + _)
    counts.show()
  }
}

using DataFrame and SparkSession : 
val linesDF = sc.textFile("file.txt").toDF("line")
 val wordsDF = linesDF.explode("line","word")((line: String) => line.split(" "))
 val wordCountDF = wordsDF.groupBy("word").count()
 wordCountDF.show()
 
 Word Count Using Spark SQL on Dataset & TempView
 val wordsDS = readFileDS.flatMap(_.split(" ")).as[String]
 wordsDS.createOrReplaceTempView("WORDS")    
 val wcounts5 = spark.sql("SELECT Value, COUNT(Value) FROM WORDS WHERE Value ='Humpty' OR Value ='Dumpty' GROUP BY Value")
 wcounts5.show

-----------------------------------------------------------------------------------

Load error messages from a log into memory, then interactively search for various patterns
lines = spark.textFile(“hdfs://...”)
errors = lines.filter(_.startsWith(“ERROR”))
messages = errors.map(_.split(‘\t’)(2))
cachedMsgs = messages.cache()
cachedMsgs.filter(_.contains(“foo”)).count
cachedMsgs.filter(_.contains(“bar”)).count
-----------------------------------------------------------------------------------

Text Search using data frame:
val textFile = sc.textFile("hdfs://...")
// Creates a DataFrame having a single column named "line"
val df = textFile.toDF("line")
val errors = df.filter(col("line").like("%ERROR%"))
// Counts all the errors
errors.count()
// Counts errors mentioning MySQL
errors.filter(col("line").like("%MySQL%")).count()
// Fetches the MySQL errors as an array of strings
errors.filter(col("line").like("%MySQL%")).collect()
------------------------------------------------------------------------
Read a table stored in a database and calculate the number of people for every age :

// Creates a DataFrame based on a table named "people"
// stored in a MySQL database.
val url =
  "jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword"
val df = sqlContext
  .read
  .format("jdbc")
  .option("url", url)
  .option("dbtable", "people")
  .load()

// Looks the schema of this DataFrame.
df.printSchema()

// Counts people by age
val countsByAge = df.groupBy("age").count()
countsByAge.show()

// Saves countsByAge to S3 in the JSON format.
countsByAge.write.format("json").save("s3a://...")


---------------------------
Turning an RDD into a Relation
//Define the schema using a case class.
case class Person(name: String, age: Int)
// Create an RDD of Person objects and register it as a table.
val people = sc.textFile("examples/src/main/resources/people.txt")
			   .map(_.split(","))
			   .map(p => Person(p(0), p(1).trim.toInt))
people.registerAsTable("people")
----------------------------------------------------------------------

Avg aggregation on RDD 
  

 val input = sc.textFile(inputFile)
 val result = input.map(_.toInt).aggregate((0, 0))(
        (acc, value) => (acc._1 + value, acc._2 + 1),
        (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
val avg = result._1 / result._2.toFloat

Note : if we use parrilze collection then to convert toInt
----------------------------------------------------------------------
How to read CSV as DataFrame
1. Do it in programmatic way
val df = spark.read
         .format("csv")
         .option("header", "true") //reading the headers
         .load("hdfs:///csv/file/dir/file.csv")

df = spark.read.format("csv").option("header", "true").load("csvfile.csv")
		 
2. You can do this SQL way as well
val df = spark.sql("SELECT * FROM csv.`hdfs://csv/file/dir/file.csv`")

----------------------------------------------------------------------
How to enable Hive Support in spark ?

1)Step 1: Move hive-site.xml from $HIVE_HOME/conf/hive-site.xml to $SPARK_HOME/conf/hive-site.xml
  Place hive-site.xml, core-site.xml, and hdfs-site.xml files in the SPARK_HOME/conf folder.

Note : if hive-site.xml is not configured that means Users who do not have an existing Hive deployment can still enable Hive support then the context automatically creates metastore_db in the current directory and creates a warehouse directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse

By default, Spark SQL uses the embedded deployment mode of a Hive metastore with a Apache Derby database.

2)Step 2: SparkSession encapuslate the HiveContext object. At the time of creation we need to enable Hive support 

// warehouseLocation points to the default location for managed databases and tables
val warehouseLocation = "file:${system:user.dir}/spark-warehouse" or  
val warehouseLocation = new File("spark-warehouse").getAbsolutePath

//hive.metastore.warehouse.dir property in hive-site.xml is deprecated since Spark 2.0.0. Use spark.sql.warehouse.dir to specify the default location of the databases in a Hive warehouse.

val spark = SparkSession
   .builder()
   .appName("SparkSessionZipsExample")
   .config("spark.sql.warehouse.dir", warehouseLocation) //Is it optional ?
   .config("hive.metastore.uri", "thrift://localhost:8080") //optional , Theory is that we provide/override run time parameters
   .enableHiveSupport()
   .getOrCreate()
   
How to configure/get run time parameters after spark session is initialized ?
//set new runtime options
spark.conf.set("spark.sql.shuffle.partitions", 6)
spark.conf.set("spark.executor.memory", "2g")

//get all settings
val configMap:Map[String, String] = spark.conf.getAll()

Why do we access Hive tables on Spark SQL and convert them into DataFrames ?
The answer is simple. For large scale projects working with petabytes of data, It is possible to reduce the time taken to process the aggregate function, if we can execute complex queries in an efficient way. Hence, we use Spark SQL, which has an in-built catalyst optimizer that processes all types of queries at a faster pace

When Spark integrates with Hive, it utilizes all the data that is stored in the warehouse which could be mysql or derby database and gives the flexibility of using spark processing engine in order to process the files 100 times faster than the Hive engine. When the client has been using Hive all over these years and has stored Terra bytes of data in the warehouse and looking for a new and effective tool to acquire fast processing of data, this is where he needs to use Apache spark in order to fetch all the data from the warehouse within seconds or minutes of time.


How to fetch Hive Hcatalog metadata in spark ?
//fetch metadata data from the catalog
spark.catalog.listDatabases.show(false)
spark.catalog.listTables.show(false)   

Saving and Reading from Hive table with SparkSession
//drop the table if exists to get around existing table error
spark.sql("DROP TABLE IF EXISTS zips_hive_table")

//Get the sqlContext from the SparkSession for support of backword compatiblity
spark.sqlContext.sql(“INSERT INTO TABLE students VALUES (‘Rahul’,’Kumar’), (‘abc’,’xyz’)”)

//save as a hive table
spark.table("zips_table").write.saveAsTable("zips_hive_table")
//make a similar query against the hive table 
val resultsHiveDF = spark.sql("SELECT city, pop, state, zip FROM zips_hive_table WHERE pop > 40000")
resultsHiveDF.show(10)

How to store data frame as Hive table ?

    // Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax
    // `USING hive`
    sql("CREATE TABLE hive_records(key int, value string) STORED AS PARQUET")
    // Save DataFrame to the Hive managed table
    val df = spark.table("src")
    df.write.mode(SaveMode.Overwrite).saveAsTable("hive_records")

How to create Hive Partition in spark
  // Turn on flag for Hive Dynamic Partitioning
    spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")
    spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")
    // Create a Hive partitioned table using DataFrame API
    df.write.partitionBy("key").format("hive").saveAsTable("hive_part_tbl")
    

How to convert data frame from Hive query ? 

Since it has schema it will come automatically.
ex1 : val dataframe = spark.sql("SELECT * FROM table")

ex 2 :val df = spark.sqlContext.table(tablename)

You can verify the schema
----------------------------------------------------------

//create a Dataset using spark.range starting from 5 to 100, with increments of 5
val numDS = spark.range(5, 100, 5)
// reverse the order and display first 5 items
numDS.orderBy(desc("id")).show(5)

   
// create a DataFrame using spark.createDataFrame from a List or Seq
val langPercentDF = spark.createDataFrame(List(("Scala", 35), ("Python", 30), ("R", 15), ("Java", 20)))
//rename the columns
val lpDF = langPercentDF.withColumnRenamed("_1", "language").withColumnRenamed("_2", "percent")
//order the DataFrame in descending order of percentage
lpDF.orderBy(desc("percent")).show(false)

// read the json file and create the dataframe
val jsonFile = args(0)
val zipsDF = spark.read.json(jsonFile)
//filter all cities whose population > 40K
zipsDF.filter(zipsDF.col("pop") > 40000).show(10)

// Creates a temporary view of the DataFrame
zipsDF.createOrReplaceTempView("zips_table")
zipsDF.cache()
val resultsDF = spark.sql("SELECT city, pop, state, zip FROM zips_table")
resultsDF.show(10)

----------------------------------------------------------
I have RDD which contains lines, I want add new column for first word and last word
val sc: SparkContext = ...
val sqlContext = new SQLContext(sc)

import sqlContext.implicits._

val extractFirstWord = udf((sentence: String) => sentence.split(" ").head)
val extractLastWord = udf((sentence: String) => sentence.split(" ").reverse.head)

val sentences = sc.parallelize(Seq("This is an example", "And this is another one", "One_word", "")).toDF("sentence")
val splits = sentences
             .withColumn("first_word", extractFirstWord(col("sentence")))
             .withColumn("last_word", extractLastWord(col("sentence")))

splits.show()

Then the output is:

+--------------------+----------+---------+
|            sentence|first_word|last_word|
+--------------------+----------+---------+
|  This is an example|      This|  example|
|And this is anoth...|       And|      one|
|            One_word|  One_word| One_word|
|                    |          |         |
+--------------------+----------+---------+

----------------------------------------------------------------------
How to add column to dataframe conditionaly ?
I am trying to take my input data:

A    B       C
--------------
4    blah    2
2            3
56   foo     3
And add a column to the end based on whether B is empty or not:

A    B       C     D
--------------------
4    blah    2     1
2            3     0
56   foo     3     1

answer : df.withcloumn("D", when($"B".isNull or $"B" === "", 0).otherwise(1))
or another way is using UDF
import org.apache.spark.sql.functions.udf
val func = UTF(b => if(b.isEmpty) 0 else 1)
df.select("$A", "$B", "$C", func($"B").as("D"))

How rename a column in dataframe
withcolumnRenamed

I have movies wanted know how many users watched each movie ?
I have departments wanted know how many emp in each department  ?
I have products wanted know how many sales happen to each product  ?

DataFrame : df.groupBy($"name").agg(count($"name") as "") show

RDD : rdd.map(f => (f._1, 1)).reduceByKey((accum, curr) => accum + curr).foreach(println(_))

SQL : select <col to Group on> , count(col to count on) from temp_table Group by <col to Group on>

How to get max/min key-value RDD from rdd pair by using value ? 

Solution1
val maxRDD = rdd.max()( Ordering[Int].on(x => x._2) ) //returns (Int, Int) tuple
val minRDD = rdd.min()( Ordering[Int].on(x => x._2) ) //returns (Int, Int) tuple

Solution2
val maxRDD = rdd.takeOrdered(1)( Ordering[Int].on(x => x._2) ) //returns Array[(Int, Int)] and defualt order is ascending
val minRDD = rdd.takeOrdered(1)( Ordering[Int].reverse.on(x => x._2) ) //returns Array[(Int, Int)]

Note : takeOrdered method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory.
   
How to get max/min key-value RDD from rdd pair by using key ? 

val maxRDD = rdd.sortByKey(true).first //returns (Int, Int) tuple
val minRDD = rdd.sortByKey(false).first //returns (Int, Int) tuple
    or 
val minRDD = rdd.sortByKey().first //returns (Int, Int) tuple

----------------
    val spark = SparkSession.builder().config("spark.master", "local").getOrCreate()
    
    val rdd = spark.sparkContext.parallelize(List( ("dep1", "emp1", "100"),
    ("dep1", "emp2", "1000"),
    ("dep3", "emp3", "300"),
    ("dep3", "emp4", "3000"),
    ("dep2", "emp5", "200"),
    ("dep2", "emp6", "2000"),
    ("dep4", "emp7", "400")
    ))    
    
   import spark.implicits._
   
   val df = rdd.toDF("dep", "emp", "sal")

Find no.of employees in each department ? or Find no.of people in each city 
Find no.of sales in each product or Find no.of transcations in each customer 
df.groupBy($"dep").agg(count($"sal"))
println(df.show())

or 
val rdd = spark.sparkContext.parallelize(List( ("dep1", "emp1", "100"),
    ("dep1", "emp2", "1000"),
    ("dep3", "emp3", "300"),
    ("dep3", "emp4", "3000"),
    ("dep2", "emp5", "200"),
    ("dep2", "emp6", "2000"),
    ("dep4", "emp7", "400")
    ))
    
   import spark.implicits._
   
   val df = rdd.toDF("dep", "emp", "sal")
   
   val w = Window.partitionBy($"dep").orderBy($"sal".desc) 
    
   val fdf = df.withColumn("maxS", row_number.over(w)).where($"maxS" === 1).drop("maxS")
   
   println(fdf.show())


+----+----+----+
| dep| emp| sal|
+----+----+----+
|dep3|emp4|3000|
|dep4|emp7| 400|
|dep2|emp6|2000|
|dep1|emp2|1000|
+----+----+----+

Find max employee sal or High salray or maximum paid emp sal or Highest top paid emp sal in each department ?
Find highest sales or top sales in each product ? Find highest transcations or top transcations in each customer ?

val gdf = df.groupBy($"dep").agg(max($"sal").alias("maxSal"), min($"sal"), sum($"sal"), avg($"sal"), sumDistinct($"sal"),count($"sal"))

Find min employee sal or Lowest salray or minimum paid emp sal or Lowest aid emp sal in each department ?
Find Lowest sales or low sales in each product ? Find Lowest transcations or low transcations in each customer ?

val gdf = df.groupBy($"dep").agg(max($"sal").alias("maxSal"), min($"sal"), sum($"sal"), avg($"sal"), sumDistinct($"sal"),count($"sal"))

Note : top or higesh or max means aggregrate max operation 
Note : low, least or Lowest or min means aggregrate min operation 
Note : no of means aggregrate count operation 
Note : Total or sum means aggregrate sum operation 




------------------------------------------------------------------
How to create UDF in spark ?

// Define a regular Scala function
val upper: String => String = _.toUpperCase

val upperUDF = spark.udf.register("upperFuncationAliasName", upper)

In spark SQL
spark.sql("SELECT upperFuncationAliasName("String")").show

In DataFrame API 
df.select(upperUDF($"x")).show














----------------------------------------------------------
spark scala log analyzer 
https://github.com/sreejithpillai/LogAnalyzerSparkScala
https://github.com/skrusche63/spark-weblog
https://alvinalexander.com/scala/analyzing-apache-access-logs-files-spark-scala

Spark SQL build in functions
http://spark.apache.org/docs/latest/api/sql/
