Few are Transformations and Few are actions. 
If operation is returning RDD then it is tranformation. If operation is returning single result value then it is action 

====================================================================================================================
RDD Operations Transformation : 
General
• map
• filter
• flatMap
• mapPartitions
• mapPartitionsWithIndex
• groupBy
• sortBy

Math/Statical 
• sample
• randomSplit

Set Theory / Relational
• union
• intersection
• subtract
• distinct
• cartesian
• zip

Data Structure / I/O
• keyBy
• zipWithIndex
• zipWithUniqueID
• zipPartitions
• coalesce
• repartition
• repartitionAndSortWithinPartitions
• pipe
----------
RDD Operations Actions : 
(aggregate, reduce, fold, collect, collectAsMap, count, max, min, sum, first, take, top, takeOrdered, foreach, keys, values, countByKey, CountByValue)
General 
• reduce
• collect
• aggregate
• fold
• first
• take
• forEach
• top
• treeAggregate
• treeReduce
• forEachPartition

Math/Statical
• count
• takeSample
• max
• min
• sum
• histogram
• mean
• variance
• stdev
• sampleVariance

set theory/Relational 
• takeOrdered

Data Structure / I/O
• saveAsTextFile
• saveAsSequenceFile
• saveAsObjectFile
• saveAsHadoopDataset
• saveAsHadoopFile
• saveAsNewAPIHadoopDataset
• saveAsNewAPIHadoopFile
====================================================================================================================
Pair RDD operation transformation 
General
• flatMapValues
• groupByKey
• reduceByKey
• reduceByKeyLocally
• foldByKey
• aggregateByKey
• sortByKey
• combineByKey

Math/Statical 
• sampleByKey

Set Theory / Relational
• cogroup (=groupWith)
• join
• subtractByKey
• fullOuterJoin
• leftOuterJoin
• rightOuterJoin

Data Structure 
• partitionBy
---------
RDD Operations Actions : 
General 
• keys
• values
• collectAsMap

Math/Statical
• countByKey
• countByValue


Experimental Actions Not recomendend to use 
• countApprox
• countApproxDistinct
• countByValueApprox
• countApproxDistinctByKey
• countByKeyApprox
• sampleByKeyExact

====================================================================================================================

Transformations : 
-------------------

Map : Returns new RDD by applying function to each element of Existing RDD
Example 1: To calculate the length of each line.
scala> lines.map(s => s.length).collect
res46: Array[Int] = Array(48, 25, 34, 5, 6, 6, 5, 5, 6)


FlatMap : Return a new RDD by first applying a function to all elements of Existing RDD, and then flattening the results
It is Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).
Example 1: Generate the Int List and compare "map" and "flatMap".

difference between Map and FlatMap
scala> val intlist = List( 1,2,3,4,5 )
scala> intlist.map(x=>List(x,x*2))
res72: List[List[Int]] = List(List(1, 2), List(2, 4), List(3, 6), List(4, 8), List(5, 10))

scala> intlist.flatMap(x=>(List(x,x*2)))
res73: List[Int] = List(1, 2, 2, 4, 3, 6, 4, 8, 5, 10)

Filter : Returns new RDD containing only the elements that satisfy a predicate
ex: filter the errors and warns from the log file
errors = lines.filter(_.startsWith(“ERROR”))

Group By: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.
val x =  sc.parallelize(Array("John", "Fred", "Anna", "James"))
val y =  x.groupBy(w => w.charAt(0))
println(y.collect().mkString(", "))
0/p is y :[('A',['Anna']),('J',['John','James']),('F',['Fred'])]
https://stackoverflow.com/questions/44404817/how-to-use-dataset-to-groupby

GroupBy vs GroupByKey ?


groupByKey() : Group values with the same key.
val inputTupplesList = sc.parallelize(List((1, 2), (3, 5), (3, 7)))
val resultGroupByKey = inputTupplesList.reduceByKey((x, y) => x + y)
println("Group values with the same key:" + resultGroupByKey.collect().mkString(",")
o/p is Group values with the same key: (1, 2), (3, 12)

reduceByKey() : Combine values with the same key.

sortByKey() :
When we apply the sortByKey() function on a dataset of (K, V) pairs, the data is sorted according to the key K in another RDD.
sortByKey() example:
    val data = spark.sparkContext.parallelize(Seq(("maths",52), ("english",75), ("science",82), ("computer",65), ("maths",85)))
    val sorted = data.sortByKey()
    sorted.foreach(println)
Note – In above code, sortByKey() transformation sort the data RDD into Ascending order of the Key(String).

coalesce() :
To avoid full shuffling of data we use coalesce() function. In coalesce() we use existing partition so that less data is shuffled. 
Using this we can cut the number of the partition. 
Suppose, we have four nodes and we want only two nodes. Then the data of extra nodes will be kept onto nodes which we kept.
    val rdd1 = spark.sparkContext.parallelize(Array("jan","feb","mar","april","may","jun"),3)
    val result = rdd1.coalesce(2)
    result.foreach(println)
Note – The coalesce will decrease the number of partitions of the source RDD to numPartitions define in coalesce argument.

join() :
The Join is database term. It combines the fields from two table using common values. join() operation in Spark is defined on pair-wise RDD. Pair-wise RDDs are RDD in which each element is in the form of tuples. Where the first element is key and the second element is the value.
The boon of using keyed data is that we can combine the data together. The join() operation combines two data sets on the basis of the key.
Join() example:
    val data = spark.sparkContext.parallelize(Array(('A',1),('b',2),('c',3)))
    val data2 =spark.sparkContext.parallelize(Array(('A',4),('A',6),('b',7),('c',3),('c',8)))
    val result = data.join(data2)
    println(result.collect().mkString(","))
Note –  The join() transformation will join two different RDDs on the basis of Key.

mapPartitions(func) :
mapPartitionWithIndex():
	
union() or ++ : Returns a new RDD that contains the union of the two source RDDs
val inputRdd1 = sc.parallelize(List(1, 2, 3))
val inputRdd2 = sc.parallelize(List(3, 5, 7))
val resultInputUnion = inputRdd1.union(inputRdd2) or inputRdd1 ++ inputRdd2
println("Union:" + resultInputUnion.collect().mkString(","))
o/p is Union: (1, 2, 3, 3, 5, 7)
Note : use distinct() to element the duplicate elements from the RDD 
val finalRDD = resultInputUnion.distinct()
o/p is (1, 2, 3, 5, 7)

intersection() :Returns a new RDD that contains the intersection of the two source RDDs
val inputRdd1 = sc.parallelize(List(1, 2, 3))
val inputRdd2 = sc.parallelize(List(3, 5, 7))
val resultIntersection = inputRdd1.intersection(inputRdd2)
println("Intersection:" + resultIntersection.collect().mkString(","))
o/p is Intersection: 3

distinct() : Returns a new RDD that contains the distinct elements of the source RDD.
val distinctInput = sc.parallelize(List(1, 2, 3, 4, 2, 1, 3, 4, 2, 5, 6))
val distinctResult = distinctInput.distinct()
println("distinct:" + distinctResult.collect().mkString(","))
o/p is distinct: 4,6,2,1,3,5

subtract() : Returns a new RDD that contains subtraction of one source RDD with another source RDD
val inputSubtract1 = sc.parallelize(List(1, 2, 3))
val inputSubtract2 = sc.parallelize(List(3, 5, 6))
val resultSub = inputSubtract1.subtract(inputSubtract2)
println("subtract:" + resultSub.collect().mkString(","))
o/p is subtract: (2,1)

zipWithIndex() : It returns new RDD with element and index
val rdd  = sc.parallelize(List("Hi", "Hello", "Bye"))
	rdd.zipWithIndex() 
	rdd.foreach(println)
	o/p is (Hi, 0)
	       (Hello, 1)
	       (Bye, 2)

Q : How to provide inital value as index instead of 0. 
Ans : 
	rdd.zipWithIndex().map { case (v, ind) =>
	  (v, ind + 8)
	}

GroupByKey vs ReduceByKey vs AggreateByKey vs CombineByKey  ?

GroupByKey() : It is just to group value based on a key and It doesn't merge the values for the key and directly the shuffle process happens. And the merging of values for each key is done after the shuffle process happens
GroupByKey can cause out of disk problems as data is sent over the network and collected on the reduce workers.

Disadvantage : Avoid groupByKey when performing an associative reductive operation on large data set

When GroupByKey called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable) pairs.
By default, the level of parallelism in the output depends on the number of partitions of the parent RDD

ex : Please see ex http://etlcode.com/index.php/blog/info/Bigdata/Apache-Spark-Difference-between-reduceByKey-groupByKey-and-combineByKey
https://www.edureka.co/community/11996/groupbykey-vs-reducebykey-in-apache-spark    

ReduceBykey(func) : It is something like grouping + aggregation. i.e first group value based on a key and then combine the values for the same key in each partition before shuffling. so that only one output value for one key at each partition is send over network and then reduce function is called again to reduce all the values from each partition to produce one final result. 

reduceByKey required combining all your values into another value with the exact same type

ReduceBykey drawback is that the values for each key has to be of same datatype. If it is different datatypes it has to explicitly converted. This drawback can be addressed using combineByKey.

Disadvantage : Avoid reduceByKey when the input and output value types are different

When ReduceBykey called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function. The function should be able to take arguments of some type and it returns same result data type

 follow groupByKey examples links

AggregateByKey() : 

combineByKey
can be used when you are combining elements but your return type differs from your input value type.
 
 
zip() : Joins two RDDs .The resulting RDD will consist of two-component tuples which are interpreted as key-value pairs
Note : Both two Joining RDDs should have same no.of paritions. And should have same no.of elements

Ex :  val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)
      val rdd2 = sc.parallelize(List("a","b","c","d","e","f","g","h","i"), 3)

val rdd13 = rdd1.zip(rdd2)
o/p is (1,a),(2,b),(3,c),(4,d),(5,e),(6,f),(7,g),(8,h),(9,i)

ZipWithIndex : Zips the elements of the RDD with its element indexes. The indexes start from 0. If the RDD is spread across multiple partitions then a spark Job is started to perform this operation.
 
def zipWithIndex(): RDD[(T, Long)]

Example
val z = sc.parallelize(Array("A", "B", "C", "D"))
val r = z.zipWithIndex 
res110: Array[(String, Long)] = Array((A,0), (B,1), (C,2), (D,3))

What is the best way to assign a sequence number into RDD ?
By using ZipWithIndex

zipWithUniqueId: This is different from zipWithIndex since just gives a unique id to each data element but the ids may not match the index number of the data element. This operation does not start a spark job even if the RDD is spread across multiple partitions.


https://trongkhoanguyen.com/spark/understand-the-spark-deployment-modes/


Actions : 
-------------------

aggregate :   def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U

zeroValue: The initialization value, for your result, in the desired format.
seqOp: The operation you want to apply to RDD records. Runs once for every record in a partition.
combOp: Defines how the resulted objects (one for every partition), gets combined.

The aggregate function allows the user to apply two different reduce functions to the RDD. The first reduce function is applied within each partition to reduce the data within each partition into a single result. The second reduce function is used to combine the different reduced results of all partitions together to arrive at one final result

The initial value is applied at both levels of reduce. So both at the intra partition reduction and across partition reduction.
Both reduce functions have to be commutative and associative

ex : Compute the sum of a list and the length of that list. Return the result in a pair of (sum, length).

collectAsMap : converts pair RDD into Map 
Note : If you have multiple values to the same key, only one value per key is preserved in the map 
(i.e last value of the key in rdd)
2)this method should use if the resulting data is expected to be small, as all the data is loaded into the driver's memory

val rdd = spark.sparkContext.parallelize(Array[(String, Int)](("A", 1), ("B", 2), ("B", 3), ("C", 4), ("C", 5), ("C", 6)))
    rdd.collectAsMap() //o/p is Map(A -> 1, C -> 6, B -> 3) 
    
ex2 : val rdd2 = spark.sparkContext.parallelize(List(1,2,3))
      val rdd3 = rdd2.zip(rdd2) //o/p is (1,1) (2,2) (3,3)
      println(rdd3.collectAsMap()) //o/p is Map(2 -> 2, 1 -> 1, 3 -> 3)


countByKey():  [will only works on Rdd key value pair (two elements tuple)]
counts the values of a RDD consisting of two-component tuples for each distinct key separately.

syntax : def countByKey(): Map[K, Long]

Example :  val c = sc.parallelize(List((3, "Gnu"), (3, "Yak"), (5, "Mouse"), (3, "Dog")), 2)
	           c.countByKey
		  o/p is res3: scala.collection.Map[Int,Long] = Map(3 -> 3, 5 -> 1)

countByValue():

Returns a map that contains all unique values of the RDD and their respective occurrence counts. 
(Warning: This operation will finally aggregate the information in a single reducer.)

Listing Variants

syntax : def countByValue(): Map[T, Long]

Example : val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1)) 
          b.countByValue
res27: scala.collection.Map[Int,Long] = Map(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> 6, 2 -> 3, 4 -> 2, 7 -> 1)

val rdd = sc.emptyRDD[Int]
rdd.reduce(_ + _)
// java.lang.UnsupportedOperationException: empty collection at   
// org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$ ...

rdd.fold(0)(_ + _)
// Int = 0

http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html

saveAsTextFile(path) 	Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.

saveAsSequenceFile(path) Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).

saveAsObjectFile(path) Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().

foreach(func) 	Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.
Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.

take(n) : we can take first n items(elements) from the RDD. 
[ It tries to cut the number of partition it accesses, so it represents a biased collection.  We cannot presume the order of the elements. ]

syntax : def take(num: Int): Array[T]

val b = sc.parallelize(List("dog", "cat", "ape", "salmon", "gnu"), 2)
b.take(2)
res18: Array[String] = Array(dog, cat)

For example, consider RDD {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “take (4)” will give result { 2, 2, 3, 4}

takeOrdered(n) : Returns the first n (smallest) elements from this RDD as defined by the specified implicit Ordering[T] and maintains the ordering. This does the opposite of top. For example:
Orders the data items of the RDD using their inherent implicit ordering function and returns the first n items as an array

sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)
// returns Array(2)

sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)
// returns Array(2, 3)

val b = sc.parallelize(List("dog", "cat", "ape", "salmon", "gnu"), 2)
b.takeOrdered(2)
res19: Array[String] = Array(ape, cat)

Note This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory.

top(n) : Returns the top n (largest) elements from this RDD as defined by the specified implicit Ordering[T] and maintains the ordering. This does the opposite of takeOrdered. For example:

syntax : def top(num: Int)(implicit ord: Ordering[T]): Array[T]

sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)
// returns Array(12)

sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)
// returns Array(6, 5)

Note : This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory

--------------------------------------------------------------------
shuffle : The process of moving the data from partition to partition in order to aggregate or join is known as shuffling. 
map-side shuffle : The aggregation/reduction that takes place before data is moved across partitions is known as a map-side shuffle.

The following operations are examples of shuffle operations for RDDs:
    groupBy/subtractByKey/foldByKey/aggregateByKey/reduceByKey
    cogroup
    any of the join transformations
    distinct
Tuning shuffle : Shuffles are tuned by a few parameters: The shuffle manager (which determines which keys go to which partitions) 
and the number/size of partitions (these are sort of interchangable)  and the number/size of executors processing your data.
When you increase the number of partitions, you inherently reduce (or in certain cases, keep the size the same) the size of each partition	
shuffle performance is the number of partitions. In practice this is done using the "repartition" or "coalesce" functions.

how-to-merge-two-rdd-to-one-rdd ?
use union

Efficient way to test if an RDD is empty
use isEmpty() or rdd.take(1).length == 0 

Add a header before text file on save in Spark
df.write
  .format("csv")
  .option("header", "true")  //adds header to file
  .save("hdfs://location/to/save/csv")
  
Apache Spark RDD filter into two RDDs
implicit class RDDOps[T](rdd: RDD[T]) {
  def partitionBy(f: T => Boolean): (RDD[T], RDD[T]) = {
    val passes = rdd.filter(f) //debug 
    val fails = rdd.filter(e => !f(e)) // Spark doesn't have filterNot //Error
    (passes, fails)
  }
} 


map vs mapPartitions?
mapPartitions : is a specialized map that is called only once for each partition.
map works the function being utilized at a per element level while mapPartitions exercises the function at the partition level.
Example Scenario : if we have 100K elements in a particular RDD partition then we will fire off the function being used by the mapping transformation 100K times when we use map.
Conversely, if we use mapPartitions then we will only call the particular function one time, but we will pass in all 100K records and get back all responses in one function call.
There will be performance gain since map works on a particular function so many times, especially if the function is doing something expensive each time that it wouldn't need to do if we passed in all the elements at once(in case of mappartitions).

So mapPartitions transformation is faster than map since it calls our function once/partition, not once/element.. 

ex : val data = sc.parallelize(List(1,2,3,4,5,6,7,8), 2)

Map:
def sumfuncmap(numbers : Int) : Int =
{
var sum = 1
return sum + numbers
}
data.map(sumfuncmap).collect
o/p returns Array[Int] = Array(2, 3, 4, 5, 6, 7, 8, 9)   

def sumfuncpartition(numbers : Iterator[Int]) : Iterator[Int] =
{
var sum = 1
while(numbers.hasNext)
{
sum = sum + numbers.next()
}
return Iterator(sum)
}
data.mapPartitions(sumfuncpartition).collect
o/p returns Array[Int] = Array(11, 27) 

def sumfuncpartition(numbers : Iterator[Int]) : Iterator[Int] =
{
var sum = 0
while(numbers.hasNext)
{
sum = sum + numbers.next()
}
return Iterator(sum)
}
data.mapPartitions(sumfuncpartition).collect
o/p returns Array[Int] = Array(10, 26) 


reduceByKey vs aggregateByKey 
scala> val nameCounts = sc.parallelize(List(("David", 6), ("Abby", 4), ("David", 5), ("Abby", 5)))
scala> nameCounts.reduceByKey((v1, v2) => v1 + v2).collect or nameCounts.reduceByKey(_+_).collect
here v1 and v2 are first value and second value 
o/p is res0: Array[(String, Int)] = Array((Abby,9), (David,11))

scala> nameCounts.aggregateByKey(0)((accum, v) => accum + k, (v1, v2) => v1 + v2).collect
res1: Array[(String, Int)] = Array((Abby,9), (David,11))

RDD sample in Spark
sample(withReplacement, fraction, seed).
fraction and seed are pretty easy to guess -- they are the fraction of elements you want to see in your sample (i.e. sample of .5 will give you a sample of initial RDD containing half of the elements). 
Seed is random number generator seed. This is important because you might want to be able to hard code the same seed for your tests so that you always get the same results in test,
but in prod code replace it with current time in milliseconds or a random number from a good entropy source
The sample transformation takes up to three parameters SparkSampling:
1.1. First is weather the sampling is done with replacement or not.
1.2. Second is the sample size as a fraction. Finally we can optionally provide a random seed.
1.3. Finally we can optionally provide a random seed.

How to increase the size of an RDD using sample with replacement
ex : I have 50 elements want to increase 200 elements
al fraction = num_new.toDouble / rdd.count()  // following my examle: num_new is 200, and rdd.count() is 35
val sampledRDD = rdd.sample(true, fraction, seed)

What is cogroup ? difference between join and cogroup transformations
cogroup : we can group the data which has same key from multiple RDDs
This is very similar to relation database operation FULL OUTER JOIN, but instead of flattening the result per line per record, it will give you the interable interface to you, 
val rdd1 = sc.makeRDD(Array(("A","1"),("B","2"),("C","3")),2)
val rdd2 = sc.makeRDD(Array(("A","a"),("C","c"),("D","d")),2)

scala> rdd1.join(rdd2).collect
res0: Array[(String, (String, String))] = Array((A,(1,a)), (C,(3,c)))

All keys that will appear in the final result is common to rdd1 and rdd2. This is similar to relation database operation INNER JOIN.

val rdd1 = sc.makeRDD(Array(("A","1"),("B","2"),("C","3")),2)
val rdd2 = sc.makeRDD(Array(("A","a"),("C","c"),("D","d")),2)

scala> var rdd3 = rdd1.cogroup(rdd2).collect
res0: Array[(String, (Iterable[String], Iterable[String]))] = Array(
(B,(CompactBuffer(2),CompactBuffer())), 
(D,(CompactBuffer(),CompactBuffer(d))), 
(A,(CompactBuffer(1),CompactBuffer(a))), 
(C,(CompactBuffer(3),CompactBuffer(c)))
)

http://www.openkb.info/2015/01/scala-on-spark-cheatsheet.html

What is Spark Executor? When SparkContext connect to a cluster manager, it acquires an Executor on nodes in the cluster. Executors are Spark processes that run computations and store the data on the worker node. The final tasks by SparkContext are transferred to executors for their execution.
What do you understand by worker node? Worker node refers to any node that can run the application code in a cluster.
What is Spark Driver? Spark Driver is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. In simple terms, driver in Spark creates SparkContext, connected to a given Spark Master. The driver also delivers the RDD graphs to Master, where the standalone cluster manager runs
 
 
What is difference between fold vs reduce  ?
Reduce is a spark action that aggregates a data set (RDD) element using a function.
That function takes two arguments and returns one.
The function must be parallel enabled.

Both are used to aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.

Main difference is fold accepts additionl parameter i.e intial value 
every reduce can be done with fold but not the other way around

Lets consider a simple example where we want to calculate "largest number greater than 15" or "15" in our RDD,
val n15GT15 = rdd1.fold(15)({ case (acc, i) => Math.max(acc, i) })
but in reduce
list.reduce{ (acc, x) => val m = math.max(acc, x); if (m > 15) m else 15 }


1)Mutable buffer

Another use case for fold is aggregation with mutable buffer. Consider following RDD:

import breeze.linalg.DenseVector

val rdd = sc.parallelize(Array.fill(100)(DenseVector(1)), 8)
Lets say we want a sum of all elements. A naive solution is to simply reduce with +:

rdd.reduce(_ + _)
Unfortunately it creates a new vector for each element. Since object creation and subsequent garbage collection is expensive it could be better to use a mutable object

2)val rdd = sc.emptyRDD[Int]
rdd.reduce(_ + _)
// java.lang.UnsupportedOperationException: empty collection at   
// org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$ ...

rdd.fold(0)(_ + _)
// Int = 0
 
val rdd1 = sc.parallelize(List(1,2,3,4,5), 3)
rdd1.fold(5)(_ + _)
This produces the output 35
 
The zeroValue is in your case added four times (one for each partition, plus one when combining the results from the partitions). So the result is:

(5 + 1) + (5 + 2 + 3) + (5 + 4 + 5) + 5 // (extra one for combining results)
 
 foldByKey vs reduceByKey vs groupByKey vs aggregateByKey vs combineByKey
 
 
 When to use GroupByKey and ReduceByKey ?
 If the operation is associative and commutative reduce (for ex : sum, count. min, max) then prefer using ReduceByKey 
 If it is not reduce operation then prefer using GroupByKey 

 https://stackoverflow.com/questions/31508083/difference-between-dataframe-in-spark-2-0-i-e-datasetrow-and-rdd-in-spark?rq=1
