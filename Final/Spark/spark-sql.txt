Simply caching Hive records as Java objects is inefficient due to high per-object overhead
Instead, Spark SQL employs column-oriented storage using arrays of primitive types

Querying Using the Scala DSL
Express queries using functions, instead of SQL strings.
// The following is the same as:
// SELECT name FROM people WHERE age >= 10 AND age <= 19

val teenagers = people.where('age >= 10)
					  .where('age <= 19)
					  .select('name)
					  
--------------------------------------
Using Parquet
// Any SchemaRDD can be stored as Parquet.
people.saveAsParquetFile("people.parquet")

// Parquet files are self-describing so the schema is preserved.
val parquetFile = sqlContext.parquetFile("people.parquet")

//Parquet files can also be registered as tables and then used in SQL 	statements.
parquetFile.registerAsTable("parquetFileâ€)

registerTempTable is a part of the 1.x API and has been deprecated in Spark 2.0.
createOrReplaceTempView and createTempView have been introduced in Spark 2.0, as a replacement for registerTempTable.
createGlobalTempView introduced in Spark 2.0

 Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax
// `USING hive`
sql("CREATE TABLE hive_records(key int, value string) STORED AS PARQUET")
// Save DataFrame to the Hive managed table
val df = spark.table("src")
df.write.mode(SaveMode.Overwrite).saveAsTable("hive_records")
