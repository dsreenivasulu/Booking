Simply caching Hive records as Java objects is inefficient due to high per-object overhead
Instead, Spark SQL employs column-oriented storage using arrays of primitive types

Querying Using the Scala DSL
Express queries using functions, instead of SQL strings.
// The following is the same as:
// SELECT name FROM people WHERE age >= 10 AND age <= 19

val teenagers = people.where('age >= 10)
					  .where('age <= 19)
					  .select('name)
					  
--------------------------------------
Using Parquet
// Any SchemaRDD can be stored as Parquet.
people.saveAsParquetFile("people.parquet")

// Parquet files are self-describing so the schema is preserved.
val parquetFile = sqlContext.parquetFile("people.parquet")

//Parquet files can also be registered as tables and then used in SQL 	statements.
parquetFile.registerAsTable("parquetFileâ€)

