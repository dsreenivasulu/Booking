createOrReplaceTempView vs createGlobalTempView
df.createOrReplaceTempView("tempViewName")
df.createGlobalTempView("tempViewName")

Drop temp table 
spark.catalog.dropTempView("tempViewName")
spark.catalog.dropGlobalTempView("tempViewName")

check temp table exists or not 
spark.catalog.tableExists("foo"))

org.apache.spark.sql.catalog package :
catalog : 
abstract class Catalog {
  def currentDatabase: String  Returns the current default database in this session
  def setCurrentDatabase(dbName: String): Unit  Sets the current default database in this session.
  def listDatabases(): Dataset[Database]   Returns a list of databases available across all sessions.

  def listTables(): Dataset[Table]  Returns a list of tables in the current database.
                                    This includes all temporary tables
  @throws[AnalysisException]("database does not exist")
  def listTables(dbName: String): Dataset[Table] Returns a list of tables in the specified database.
                                  This includes all temporary tables.     

 
  def listFunctions(): Dataset[Function]  Returns a list of functions registered in the current database
                                          This includes all temporary functions 
  @throws[AnalysisException]("database does not exist")
  def listFunctions(dbName: String): Dataset[Function]  Returns a list of functions registered in the specified database.
                                                        This includes all temporary functions 
  
  @throws[AnalysisException]("table does not exist")
  def listColumns(tableName: String): Dataset[Column] Returns a list of columns for the given table in the current database or
                                                       the given temporary table.
  
  @throws[AnalysisException]("database or table does not exist")
  def listColumns(dbName: String, tableName: String): Dataset[Column] Returns a list of columns for the given table in the specified database


  @throws[AnalysisException]("database does not exist") Get the database with the specified name
  def getDatabase(dbName: String): Database

  /**
   * Get the table or view with the specified name. This table can be a temporary view or a
   * table/view in the current database. This throws an AnalysisException when no Table
   * can be found.
   */
  @throws[AnalysisException]("table does not exist")  Get the table or view with the specified name in the current database
  def getTable(tableName: String): Table

  /**
   * Get the table or view with the specified name in the specified database. This throws an
   * AnalysisException when no Table can be found.
   */
  @throws[AnalysisException]("database or table does not exist")
  def getTable(dbName: String, tableName: String): Table

  /**
   * Get the function with the specified name. This function can be a temporary function or a
   * function in the current database. This throws an AnalysisException when the function cannot
   * be found.
 
   */
  @throws[AnalysisException]("function does not exist")
  def getFunction(functionName: String): Function

  /**
   * Get the function with the specified name. This throws an AnalysisException when the function
   * cannot be found.
   */
  @throws[AnalysisException]("database or function does not exist")
  def getFunction(dbName: String, functionName: String): Function

  /**
   * Check if the database with the specified name exists
   */
  def databaseExists(dbName: String): Boolean

  /**
   * Check if the table or view with the specified name exists. This can either be a temporary
   * view or a table/view in the current database.
   */
  def tableExists(tableName: String): Boolean

  /**
   * Check if the table or view with the specified name exists in the specified database.
   */
  def tableExists(dbName: String, tableName: String): Boolean

  /**
   * Check if the function with the specified name exists. This can either be a temporary function
   * or a function in the current databas
   */
  def functionExists(functionName: String): Boolean

  /**
   * Check if the function with the specified name exists in the specified database.
  def functionExists(dbName: String, functionName: String): Boolean

  /**
   * :: Experimental ::
   * Creates an external table from the given path and returns the corresponding DataFrame.
   * It will use the default data source configured by spark.sql.sources.default.
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(tableName: String, path: String): DataFrame

  /**
   * :: Experimental ::
   * Creates an external table from the given path based on a data source
   * and returns the corresponding DataFrame.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(tableName: String, path: String, source: String): DataFrame

  /**
   * :: Experimental ::
   * Creates an external table from the given path based on a data source and a set of options.
   * Then, returns the corresponding DataFrame.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(
      tableName: String,
      source: String,
      options: java.util.Map[String, String]): DataFrame

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Creates an external table from the given path based on a data source and a set of options.
   * Then, returns the corresponding DataFrame.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(
      tableName: String,
      source: String,
      options: Map[String, String]): DataFrame

  /**
   * :: Experimental ::
   * Create an external table from the given path based on a data source, a schema and
   * a set of options. Then, returns the corresponding DataFrame.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(
      tableName: String,
      source: String,
      schema: StructType,
      options: java.util.Map[String, String]): DataFrame

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Create an external table from the given path based on a data source, a schema and
   * a set of options. Then, returns the corresponding DataFrame.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createExternalTable(
      tableName: String,
      source: String,
      schema: StructType,
      options: Map[String, String]): DataFrame

  /**
   * Drops the local temporary view with the given view name in the catalog.
   * If the view has been cached before, then it will also be uncached.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * Note that, the return type of this method was Unit in Spark 2.0, but changed to Boolean
   * in Spark 2.1.
   *
   * @param viewName the name of the view to be dropped.
   * @return true if the view is dropped successfully, false otherwise.
   * @since 2.0.0
   */
  def dropTempView(viewName: String): Boolean

  /**
   * Drops the global temporary view with the given view name in the catalog.
   * If the view has been cached before, then it will also be uncached.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `_global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM _global_temp.view1`.
   *
   * @param viewName the name of the view to be dropped.
   * @return true if the view is dropped successfully, false otherwise.
   * @since 2.1.0
   */
  def dropGlobalTempView(viewName: String): Boolean

  /**
   * Returns true if the table is currently cached in-memory.
   *
   * @since 2.0.0
   */
  def isCached(tableName: String): Boolean

  /**
   * Caches the specified table in-memory.
   *
   * @since 2.0.0
   */
  def cacheTable(tableName: String): Unit

  /**
   * Removes the specified table from the in-memory cache.
   *
   * @since 2.0.0
   */
  def uncacheTable(tableName: String): Unit

  /**
   * Removes all cached tables from the in-memory cache.
   *
   * @since 2.0.0
   */
  def clearCache(): Unit

  /**
   * Invalidate and refresh all the cached metadata of the given table. For performance reasons,
   * Spark SQL or the external data source library it uses might cache certain metadata about a
   * table, such as the location of blocks. When those change outside of Spark SQL, users should
   * call this function to invalidate the cache.
   *
   * If this table is cached as an InMemoryRelation, drop the original cached version and make the
   * new version cached lazily.
   *
   * @since 2.0.0
   */
  def refreshTable(tableName: String): Unit

  /**
   * Invalidate and refresh all the cached data (and the associated metadata) for any dataframe that
   * contains the given data source path. Path matching is by prefix, i.e. "/" would invalidate
   * everything that is cached.
   *
   * @since 2.0.0
   */
  def refreshByPath(path: String): Unit
}
------------------------------------------------------------------------------------------------------------
org.apache.spark.sql.catalog package :
catalog usage to access metadata
Spark SQL provides a minimalist API known as Catalog API for data processing applications to query and use the metadata in the applications. The Catalog API exposes a catalog abstraction with many databases in it. For the regular SparkSession, it will have only one database, namely default. But if Spark is used with Hive, then the entire Hive meta store will be available through the Catalog API

One major difference between the two contexts is that the SQLContext makes use of a transient catalog to store metadata, while the HiveContext supports persistent metadata storage in the Hive Metastore (see the figure below). If you therefore create a table using the SQLContext, the table metadata goes out of scope at the end of the Spark session, and if a new session is started the table needs to be created again. In contrast, the HiveContext persists table metadata in the Hive metastore. Therefore, tables that have been created with the HiveContext "survive" session restarts.

column : A column in Spark, as returned by listColumns method in Catalog.
----
name name of the column.

description description of the column.

dataType data type of the column.

nullable whether the column is nullable.

isPartition whether the column is a partition column.

isBucket whether the column is a bucket column.

Database : A database in Spark, as returned by the listDatabases method defined in Catalog.
----

description: String description of the database.

val locationUri: String path (in the form of a uri) to data files.

val name: String name of the database.

Function : A user-defined function in Spark, as returned by listFunctions method in Catalog.
------
val
className: String
 Permalink
the fully qualified class name of the function.

val
database: String
name of the database the function belongs to.

val
description: String
description of the function; description can be null.

val
isTemporary: Boolean
whether the function is a temporary function or not.

val
name: String
name of the function.

Table : A table in Spark, as returned by the listTables method in Catalog.
-----
val
database: String
 Permalink
name of the database the table belongs to.

val
description: String
description of the table.

val
isTemporary: Boolean
whether the table is a temporary table.

val
name: String
name of the table.

val
tableType: String
type of the table (e.g.


