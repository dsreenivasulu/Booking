spark-submit
-------------
The spark-submit is used to launch spark applications on a cluster (present./bin/spark-submit)

spark-submit is uniform interface, used in all the spark's supported cluster managers to launch the spark application

To generate the application JAR $ mvn package

What happens when a Spark Job is submitted?
=====================================
https://stackoverflow.com/questions/24637312/spark-driver-in-apache-spark
When a client submits a spark application, the driver implicitly converts the code containing transformations and actions into a logical directed acyclic graph (DAG). 
At this stage, the driver program also performs certain optimizations like pipelining transformations and then it converts the logical DAG into physical execution plan with set of stages. 
After creating the physical execution plan, it creates small physical execution units referred to as tasks under each stage. 
Then tasks are bundled to be sent to the Spark Cluster.

The driver program then talks to the cluster manager and negotiates for resources.The cluster manager then launches executors on the worker nodes on behalf of the driver. 
At this point the driver sends tasks to the cluster manager based on data placement. Before executors begin execution, they register themselves with the driver program so that the driver has holistic view of all the executors. 
Now executors start executing the various tasks assigned by the driver program. At any point of time when the spark application is running, the driver program will monitor the set of executors that run. 
Driver program in the spark architecture also schedules future tasks based on data placement by tracking the location of cached data. 
When driver programs main () method exits or when it call the stop () method of the Spark Context, it will terminate all the executors and release the resources from the cluster manager.


In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one

Catalyst optimizer :
=====================
The core of Spark SQL is catalyst optimizer. It is based on functional programming construct in Scala.
catalyst optimizer, optimize all the queries written both in spark sql and dataframe API. 
This optimizer makes queries run much faster than their RDD counterparts.This increases the performance of the system.

Internally, Catalyst contains the tree and the set of rules to manipulate the tree.

Catalyst Optimizer supports both rule-based and cost-based optimization.
In rule-based optimization,set of rules are used to determine how to execute the query. 
While the cost based optimization finds the most suitable way to carry out SQL statement. 
In cost-based optimization, multiple plans are generated using rules and then their cost is computed.

It offers a general framework for transforming trees in four phases
analysis : The analysis phase involved looking at a SQL query or a DataFrame, creating a logical plan out of it, 
which is still unresolved (the columns referred may not exist or may be of wrong datatype) and then resolving this plan using the Catalog object
Logical plan optimization : In this phase, the standard rule-based optimization is applied to the logical plan. 
Physical planning : In this phase, one or more physical plan is formed from the logical plan
Runtime Code generation : The final phase of query optimization involves generating Java bytecode to run on each machine.
(catalyst uses the special Scala feature, “Quasiquotes”)

Catalyst optimizer has two primary goals:
Make adding new optimization techniques easy
Enable external developers to extend the optimizer	

In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one		
---------------------------------------------------------------------------------
How to enable debug logs in spark application 
go to etc/spark/conf/log4j.properties then change the log4j.rootCategory=DEGBUG, console

How to monitor spark application ? How to check what goes went wrong in spark application ? How to check the running status of spark application ?
cluster manager logs and the Spark web application UI. 

YARN Logs 
Yarn-site.xml we can set location by using yarn.nodemanager.log-dirs and default is /etc/hadoop/conf/log4j.properties
/var/log / - do a ls 
you should see folders.

hadoop-0.20-mapreduce 
hadoop-hdfs
hadoop-mapreduce 
hadoop-yarn

YARN logs stored in /var/log/hadoop-yarn
We can also get the webUI of cloudera manager 

Spark web application UI will have the following information
A list of stages and tasks.
The execution directed acyclic graph (DAG) for each job.
A summary of RDD sizes and memory usage.
Environment - runtime information, property settings, library paths.
Information about Spark SQL jobs.

If log aggregation is turned on (with the yarn.log-aggregation-enable config), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the yarn logs command.
YARN Application logs 
yarn logs -applicationId <app ID>

YARN configs yarn.nodemanager.remote-app-log-dir and yarn.nodemanager.remote-app-log-dir-suffix
We can also view HDFS using the HDFS shell or API. Note logs are copied to HDFS

The logs are also available on the Spark Web UI under the Executors Tab. You need to have both the Spark history server and the MapReduce history server running and configure yarn.log.server.url in yarn-site.xml properly. The log URL on the Spark history server UI will redirect you to the MapReduce history server to show the aggregated logs.

When log aggregation isn’t turned on, logs are retained locally on each machine under YARN_APP_LOGS_DIR, which is usually configured to /tmp/logs or $HADOOP_HOME/logs/userlogs depending on the Hadoop version and installation. Viewing logs for a container requires going to the host that contains them and looking in this directory. Subdirectories organize log files by application ID and container ID. The logs are also available on the Spark Web UI under the Executors Tab and doesn’t require running the MapReduce history server.

If you need a reference to the proper location to put log files in the YARN so that YARN can properly display and aggregate them, use spark.yarn.app.container.log.dir in your log4j.properties. For example, log4j.appender.file_appender.File=${spark.yarn.app.container.log.dir}/spark.log. 



------------------------------------------------------------------------------
Steps for spark application 
-Write spark application like word count program
-Generate a JAR file mvn package. It will create a JAR in target directory
-Run in cmd prompt 
Local Mode
$ spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master local \
--deploy-mode client \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

YARN Client Mode 
spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master yarn \
--deploy-mode client \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

YARN cluster Mode :
spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master yarn \
--deploy-mode cluster  \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

syntax : spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]

spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    lib/spark-examples*.jar \
    10

Adding other JARS : 
spark-submit --class my.main.Class \
    --master yarn \
    --deploy-mode cluster \
    --jars my-other-jar.jar,my-other-other-jar.jar \
    my-main-jar.jar \
    app_arg1 app_arg2
	
SPARK_HOME/conf/spark-defaults.conf
Simple example : https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/
https://console.bluemix.net/docs/services/AnalyticsforApacheSpark/spark_submit_example.html#example-running-a-spark-application-with-optional-parameters
https://stackoverflow.com/questions/39365552/loading-properties-with-spark-submit
https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-submit.adoc	

YARN Modes ?
There are two deploy modes that can be used to launch Spark applications on YARN. 
In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.
Cluster mode: everything runs inside the cluster. You can start a job from your laptop and the job will continue running even if you close your computer. In this mode, the Spark Driver is encapsulated inside the YARN Application Master.

Client mode the Spark driver runs on a client, such as your laptop. If the client is shut down, the job fails. Spark Executors still run on the cluster, and to schedule everything, a small YARN Application Master is created.

Client mode is well suited for interactive jobs, but applications will fail if the client stops. For long running jobs, cluster mode is more appropriate.
	