spark-submit
-------------
The spark-submit is used to launch spark applications on a cluster (present./bin/spark-submit)

spark-submit is uniform interface, used in all the spark's supported cluster managers to launch the spark application

To generate the application JAR $ mvn package

What happens when a Spark Job is submitted?
=====================================
https://stackoverflow.com/questions/24637312/spark-driver-in-apache-spark
When a client submits a spark application, the driver implicitly converts the code containing transformations and actions into a logical directed acyclic graph (DAG). 
At this stage, the driver program also performs certain optimizations like pipelining transformations and then it converts the logical DAG into physical execution plan with set of stages. 
After creating the physical execution plan, it creates small physical execution units referred to as tasks under each stage. 
Then tasks are bundled to be sent to the Spark Cluster.

The driver program then talks to the cluster manager and negotiates for resources.The cluster manager then launches executors on the worker nodes on behalf of the driver. 
At this point the driver sends tasks to the cluster manager based on data placement. Before executors begin execution, they register themselves with the driver program so that the driver has holistic view of all the executors. 
Now executors start executing the various tasks assigned by the driver program. At any point of time when the spark application is running, the driver program will monitor the set of executors that run. 
Driver program in the spark architecture also schedules future tasks based on data placement by tracking the location of cached data. 
When driver programs main () method exits or when it call the stop () method of the Spark Context, it will terminate all the executors and release the resources from the cluster manager.


In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one

What is DAG and explain ?
From Graph Theory, DAG is a Directed acyclic graph in which there are no cycles or loops, i.e., if you start from a node along the directed branches, you would never visit the already visited node by any chance.

DAG in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDD

At a high level, When an Action is called on RDD spark creates the logical DAG from the operator graph(Spark creates an operator graph when you enter your code) and submit to the DAG Scheduler

The DAG scheduler divides operators into stages of tasks. A stage contains task based on the partition of the input data. The DAG scheduler pipelines operators(transformations) together. i.e At higher level, two type of RDD transformations can be applied: narrow transformation (e.g. map(), filter() etc.) and wide transformation (e.g. reduceByKey()). Narrow transformation does not require the shuffling of data across a partition, the narrow transformations will be grouped into single stage while in wide transformation require the data shuffling. Hence, Wide transformation results in stage boundaries.

The final result of a DAG scheduler is a set of stages with physical execution plan.
After creating the physical execution plan, it creates small physical execution units referred to as tasks under each stage
A stage contains task based on the partition of the input data.

The Stages are passed on to the Task Scheduler.The task scheduler launches tasks via cluster manager (Spark Standalone/Yarn/Mesos). The task scheduler doesn't know about dependencies of the stages.

Finally the Worker executes the tasks. (A new JVM is started per job.) The worker knows only about the code that is passed to it.

Each RDD maintains a pointer to one or more parent along with metadata about what type of relationship it has with the parent. For example, if we call val b=a.map() on an RDD, the RDD b keeps a reference to its parent RDD a, that’s an RDD lineage

Catalyst optimizer :
=====================
The core of Spark SQL is catalyst optimizer. It is based on functional programming construct in Scala.
catalyst optimizer, optimize all the queries written both in spark sql and dataframe API. 
This optimizer makes queries run much faster than their RDD counterparts.This increases the performance of the system.

Internally, Catalyst contains the tree and the set of rules to manipulate the tree.

Catalyst Optimizer supports both rule-based and cost-based optimization.
In rule-based optimization,set of rules are used to determine how to execute the query. 
While the cost based optimization finds the most suitable way to carry out SQL statement. 
In cost-based optimization, multiple plans are generated using rules and then their cost is computed.

It offers a general framework for transforming trees in four phases
analysis : The analysis phase involved looking at a SQL query or a DataFrame, creating a logical plan out of it, 
which is still unresolved (the columns referred may not exist or may be of wrong datatype) and then resolving this plan using the Catalog object
Logical plan optimization : In this phase, the standard rule-based optimization is applied to the logical plan. 
Physical planning : In this phase, one or more physical plan is formed from the logical plan
Runtime Code generation : The final phase of query optimization involves generating Java bytecode to run on each machine.
(catalyst uses the special Scala feature, “Quasiquotes”)

Catalyst optimizer has two primary goals:
Make adding new optimization techniques easy
Enable external developers to extend the optimizer	

In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one		
---------------------------------------------------------------------------------
How to enable debug logs in spark application 
go to etc/spark/conf/log4j.properties then change the log4j.rootCategory=DEGBUG, console

How to monitor spark application ? How to check what goes went wrong in spark application ? How to check the running status of spark application ?
cluster manager logs and the Spark web application UI. 

YARN Logs 
Yarn-site.xml we can set location by using yarn.nodemanager.log-dirs and default is /etc/hadoop/conf/log4j.properties
/var/log / - do a ls 
you should see folders.

hadoop-0.20-mapreduce 
hadoop-hdfs
hadoop-mapreduce 
hadoop-yarn

YARN logs stored in /var/log/hadoop-yarn
We can also get the webUI of cloudera manager 

Spark web application UI will have the following information
A list of stages and tasks.
The execution directed acyclic graph (DAG) for each job.
A summary of RDD sizes and memory usage.
Environment - runtime information, property settings, library paths.
Information about Spark SQL jobs.

If log aggregation is turned on (with the yarn.log-aggregation-enable config), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the yarn logs command.
YARN Application logs 
yarn logs -applicationId <app ID>

YARN configs yarn.nodemanager.remote-app-log-dir and yarn.nodemanager.remote-app-log-dir-suffix
We can also view HDFS using the HDFS shell or API. Note logs are copied to HDFS

The logs are also available on the Spark Web UI under the Executors Tab. You need to have both the Spark history server and the MapReduce history server running and configure yarn.log.server.url in yarn-site.xml properly. The log URL on the Spark history server UI will redirect you to the MapReduce history server to show the aggregated logs.

When log aggregation isn’t turned on, logs are retained locally on each machine under YARN_APP_LOGS_DIR, which is usually configured to /tmp/logs or $HADOOP_HOME/logs/userlogs depending on the Hadoop version and installation. Viewing logs for a container requires going to the host that contains them and looking in this directory. Subdirectories organize log files by application ID and container ID. The logs are also available on the Spark Web UI under the Executors Tab and doesn’t require running the MapReduce history server.

If you need a reference to the proper location to put log files in the YARN so that YARN can properly display and aggregate them, use spark.yarn.app.container.log.dir in your log4j.properties. For example, log4j.appender.file_appender.File=${spark.yarn.app.container.log.dir}/spark.log. 



------------------------------------------------------------------------------
Steps for spark application 
-Write spark application like word count program
-Generate a JAR file mvn package. It will create a JAR in target directory
-Run in cmd prompt 
Local Mode
$ spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master local \
--deploy-mode client \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

YARN Client Mode 
spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master yarn \
--deploy-mode client \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

YARN cluster Mode :
spark-submit --class com.cloudera.sparkwordcount.SparkWordCount \
--master yarn \
--deploy-mode cluster  \
--executor-memory 1g \
--name wordcount \
--conf "spark.app.id=wordcount" \
sparkwordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://namenode_host:8020/path/to/inputfile.txt 2

syntax : spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]

spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    lib/spark-examples*.jar \
    10

Adding other JARS : 
spark-submit --class my.main.Class \
    --master yarn \
    --deploy-mode cluster \
    --jars my-other-jar.jar,my-other-other-jar.jar \
    my-main-jar.jar \
    app_arg1 app_arg2
	
SPARK_HOME/conf/spark-defaults.conf
Simple example : https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/
https://console.bluemix.net/docs/services/AnalyticsforApacheSpark/spark_submit_example.html#example-running-a-spark-application-with-optional-parameters
https://stackoverflow.com/questions/39365552/loading-properties-with-spark-submit
https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-submit.adoc	

YARN Modes ?
There are two deploy modes that can be used to launch Spark applications on YARN. 
In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.
Cluster mode: everything runs inside the cluster. You can start a job from your laptop and the job will continue running even if you close your computer. In this mode, the Spark Driver is encapsulated inside the YARN Application Master.

Client mode the Spark driver runs on a client, such as your laptop. If the client is shut down, the job fails. Spark Executors still run on the cluster, and to schedule everything, a small YARN Application Master is created.

Client mode is well suited for interactive jobs, but applications will fail if the client stops. For long running jobs, cluster mode is more appropriate.
	
