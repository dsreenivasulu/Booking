
What is broadcast variable?
broadcast variable responsible to broadcast the variable data into all the executor nodes in the cluster
When we call broadcast method, then the variable data copied to executors only once (instead of copying the variable data every time with executer task)
that means broadcast variable store and cached the data in all the executors only once 
broadcast variable would be loaded into the memory on the executor nodes only when it is required
broadcast variable are immutable that means it is read-only variable cached on each machine
broadcast variable useful when the Task across multiple stages needs the same data
Note: Spark distributes board cost variable efficiently to reduce communication cost.  (it is like distributed cache of Hadoop)
[Quite often we have to send certain data to every node. The most efficient way of sending the data to all of the nodes is by the use of broadcast variables.
Even though we could refer an internal variable which will get copied everywhere but the broadcast variable is far more efficient. 
It would be loaded into the memory on the nodes only where it is required and when it is required not all the time.]

We can unpersist the broadcast variable 
ex : variableName.unpersist() : Asynchronously deletes cached copies of this broadcast on the executors.

We can destroy the broadcast variable 
Before calling destroy method we need to call unpersist method
ex : variableName.destroy() : Destroys all data and metadata related to this broadcast variable.

Note : 1)Once a broadcast variable has been destroyed, it cannot be used again. 
       2)If you try to destroy a broadcast variable more than once, you will see the following SparkException 
          org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27)
Example 1 :		  val hoods = Seq((1, "Mission"), (2, "SOMA"), (3, "Sunset"), (4, "Haight Ashbury"))
val checkins = Seq((234, 1),(567, 2), (234, 3), (532, 2), (234, 4))
val hoodsRdd = sc.parallelize(hoods)
val checkRdd = sc.parallelize(checkins)
val broadcastedHoods = sc.broadcast(hoodsRdd.collectAsMap())
val checkinsWithHoods = checkRdd.mapPartitions({row =>
 row.map(x => (x._1, x._2, broadcastedHoods.value.getOrElse(x._2, -1)))
}, preservesPartitioning = true)
Example 2: var commonWords = Array("a", "an", "the", "of", "at", "is", "am","are","this","that","at", "in", "or", "and", "or", "not", "be", "for", "to", "it")
val commonWordsMap = collection.mutable.Map[String, Int]()
for(word <- commonWords){
    commonWordsMap(word) = 1
}
var commonWordsBC = sc.broadcast(commonWordsMap)

var file = sc.textFile("/data/mr/wordcount/input/big.txt")
def toWords(line:String):Array[String] = {  
    var words = line.split(" ")
    var output = Array[String]();
    for(word <- words){
        if(! (commonWordsBC.value contains word.toLowerCase.trim.replaceAll("[^a-z]",""))) output = output :+ word;
    }
    return output;
}
var uncommonWords = file.flatMap(toWords)
uncommonWords.take(100)

Example 3 : scala> val b = sc.broadcast(1)
b: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)
scala> b.value
res0: Int = 1
scala> b.unpersist
scala> b.destroy


What is accumulator?
Accumulators are variables that are used to perform an associative and commutative operation in parallel across the executors in cluster. 
They can be used to implement counters (as in MapReduce) or sums.
Accumulator are write-only variables for executors and An accumulator a kind of central variable to which every node can send data.
And then Accumulator variable value can be read by the driver
we can create named or unnamed accumulators. a named accumulator will display in the web UI. 
Tracking accumulators in the UI can be useful for understanding the progress of running stages 
In Spark, We have build in accumulators of numeric types and we can also add support for new types.
An accumulator is a good way to continuously gather data from a Spark process such as the progress of an application.

We can create built-in accumulators for longs, doubles, or collections or 
register custom accumulators using the SparkContext.register methods

An accumulator can have an optional name that you can specify when creating an accumulator.
val counter = sc.longAccumulator("counter")

Accumulators are applicable to any operation which are,
1. Commutative -> f(x, y) = f(y, x), and
2. Associative -> f(f(x, y), z) = f(f(x, z), y) = f(f(y, z), x)
For example, sum and max functions satisfy the above conditions whereas average does not.

Note : Accumulators used inside functions like map() or filter() wont get executed unless some action happen on the RDD.
    Spark guarantees to update accumulators inside actions only once. So even if a task is restarted and the lineage is recomputed, the accumulators will be updated only once.
    Spark does not guarantee Accumulators inside transformations. So if a task is restarted and the lineage is recomputed, there are chances of undesirable side effects when the accumulators will be updated more than once.
    To be on the safe side, always use accumulators inside actions ONLY.
Spark accumulators are similar to Hadoop counters

Example1:	
var file = sc.textFile("/data/mr/wordcount/input/")
var numBlankLines = sc.accumulator(0)

def toWords(line:String): Array[String] = {
  if(line.length == 0) {numBlankLines += 1}
  return line.split(" ");
}

var words = file.flatMap(toWords)
words.saveAsTextFile("words3")
printf("Blank lines: %d", numBlankLines.value)
------------------------------------------------------
Relatime example : We have used to know API diagnosis like how many records are corrupted or how many times a particular library API was called.

object Boot {
 
 import utils.Utils._
 
 def main(args: Array[String]): Unit = {
 
   val sparkConf = new SparkConf(true)
     .setMaster("local[2]")
     .setAppName("SparkAnalyzer")
 
   val sparkContext = new SparkContext(sparkConf)
 
   /**
     * Defining list of all HTTP status codes divided into status groups
     * This list is read only, and it is used for parsing access log file in order to count status code groups
     *
     * This example of broadcast variable shows how broadcast value
     */
   val httpStatusList = sparkContext broadcast populateHttpStatusList
 
   /**
     * Definition of accumulators for counting specific HTTP status codes
     * Accumulator variable is used because of all the updates to this variable in every executor is relayed back to the driver.
     * Otherwise they are local variable on executor and it is not relayed back to driver
     * so driver value is not changed
     */
   val httpInfo = sparkContext accumulator(0, "HTTP 1xx")
   val httpSuccess = sparkContext accumulator(0, "HTTP 2xx")
   val httpRedirect = sparkContext accumulator(0, "HTTP 3xx")
   val httpClientError = sparkContext accumulator(0, "HTTP 4xx")
   val httpServerError = sparkContext accumulator(0, "HTTP 5xx")
 
   /**
     * Iterate over access.log file and parse every line
     * for every line extract HTTP status code from it and update appropriate accumulator variable
     */
   sparkContext.textFile(getClass.getResource("/access.log").getPath, 2).foreach { line =>
     httpStatusList.value foreach {
       case httpInfoStatus: HttpInfoStatus if (AccessLogParser.parseHttpStatusCode(line).equals(Some(httpInfoStatus))) => httpInfo += 1
       case httpSuccessStatus: HttpSuccessStatus if (AccessLogParser.parseHttpStatusCode(line).equals(Some(httpSuccessStatus))) => httpSuccess += 1
       case httpRedirectStatus: HttpRedirectStatus if (AccessLogParser.parseHttpStatusCode(line).equals(Some(httpRedirectStatus))) => httpRedirect += 1
       case httpClientErrorStatus: HttpClientErrorStatus if (AccessLogParser.parseHttpStatusCode(line).equals(Some(httpClientErrorStatus))) => httpClientError += 1
       case httpServerErrorStatus: HttpServerErrorStatus if (AccessLogParser.parseHttpStatusCode(line).equals(Some(httpServerErrorStatus))) => httpServerError += 1
       case _ =>
     }
   }
 
   println("########## START ##########")
   println("Printing HttpStatusCodes result from parsing access log")
   println(s"HttpStatusInfo : ${httpInfo.value}")
   println(s"HttpStatusSuccess : ${httpSuccess.value}")
   println(s"HttpStatusRedirect : ${httpRedirect.value}")
   println(s"HttpStatusClientError : ${httpClientError.value}")
   println(s"HttpStatusServerError : ${httpServerError.value}")
   println("########## END ##########")
 
   sparkContext.stop()
 }
 
}

object Utils {
 
  private val httpStatuses = List(
    "100", "101", "103",
    "200", "201", "202", "203", "204", "205", "206",
    "300", "301", "302", "303", "304", "305", "306", "307", "308",
    "400", "401", "402", "403", "404", "405", "406", "407", "408", "409", "410", "411", "412", "413", "414", "415", "416", "417",
    "500", "501", "502", "503", "504", "505", "511"
  )
 
  def populateHttpStatusList(): List[HttpStatus] = {
      httpStatuses map createHttpStatus
  }
 
  def createHttpStatus(status: String): HttpStatus = status match {
    case status if (status.startsWith("1")) => HttpInfoStatus(status)
    case status if (status.startsWith("2")) => HttpSuccessStatus(status)
    case status if (status.startsWith("3")) => HttpRedirectStatus(status)
    case status if (status.startsWith("4")) => HttpClientErrorStatus(status)
    case status if (status.startsWith("5")) => HttpServerErrorStatus(status)
  }
 
}

object AccessLogParser extends Serializable {
  import Utils._
 
  private val ddd = "\\d{1,3}"
  private val ip = s"($ddd\\.$ddd\\.$ddd\\.$ddd)?"
  private val client = "(\\S+)"
  private val user = "(\\S+)"
  private val dateTime = "(\\[.+?\\])"
  private val request = "\"(.*?)\""
  private val status = "(\\d{3})"
  private val bytes = "(\\S+)"
  private val referer = "\"(.*?)\""
  private val agent = "\"(.*?)\""
  private val accessLogRegex = s"$ip $client $user $dateTime $request $status $bytes $referer $agent"
  private val p = Pattern.compile(accessLogRegex)
 
  /**
    * Extract HTTP status code and create HttpStatus instance for given status code
    */
  def parseHttpStatusCode(logLine: String): Option[HttpStatus] = {
    val matcher = p.matcher(logLine)
    if(matcher.find) {
      Some(createHttpStatus(matcher.group(6)))
    }
    else {
      None
    }
  }
 
}
access.log 

31.184.238.202 - - [18/Jun/2016:17:01:05 -0700] "GET /logs/access.log HTTP/1.1" 200 1076459 "http://gravatar.com/buysovaldi400mgonlinesafely" "Opera/9.80 (Windows NT 6.1; WOW64) Presto/2.12.388 Version/12.15" "redlug.com"
114.37.233.208 - - [18/Jun/2016:17:05:04 -0700] "GET /logs/access.log HTTP/1.1" 200 209 "http://redlug.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36" "redlug.com"
31.184.238.202 - - [18/Jun/2016:17:10:19 -0700] "GET /logs/access.log HTTP/1.1" 200 467 "http://www.theknot.com/wedding/neurontin-ordering" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrom