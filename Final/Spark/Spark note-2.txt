SparkSession :
In Spark 2.0, SparkSession represents a unified point for manipulating data in Spark. It minimizes the number of different contexts a developer has to use while working with Spark. SparkSession replaces multiple context objects, such as the SparkContext, SQLContext, and HiveContext. These contexts are now encapsulated within the SparkSession object.

In Spark programs, we use the builder design pattern to instantiate a SparkSession object. However, in the REPL environment (that is, in a Spark shell session), the SparkSession is automatically created and made available to you via an instance object called Spark.

How to read xml file in spark  ?
val df = sqlContext.read.format("com.databricks.spark.xml")
						.option("rowTag", "<root element tag>")
						.load("xml path")
rowTag - defines xml tag that is a record indicator.
ex : val df = sqlContext.read.format("com.databricks.spark.xml")
						.option("rowTag", "foo")
						.load("bar.xml")
Another way of two options :
1)if files are small xml files ,load using SparkContext.wholeTextFiles. 
It loads data as RDD[(String, String)] where the the first element is path and the second file content. 
Then you parse each file individually like in a local mode.

2)For large xml files you can use XmlInputFormat  (Hadoop input formats provided by Mahout)

How the data source can be divided in spark ?
Data sources in Apache Spark can be divided into three groups:
    structured data like Avro files, Parquet files, ORC files, Hive tables, JDBC sources
    semi-structured data like JSON, CSV or XML
    unstructured data: log lines, images, binary files
-Advantage of structured data is we know the schema in advance (field names, their types and “nullability”). 
-Advantage of semi-structured data is they are all easy to read and they can be modified with lots of tools (starting with notepads, ending with Excel or jq command). 
The schema of semi-structured formats is not strict. That means we don’t know what field names and data types to expect until we read at least part of the file. 
Well, in CSV we may have column names in the first row, but this is not enough in most cases
That’s why each time you open a JSON/CSV/XML dataset in Spark using the simplest API, you wait some time and see jobs executed in WebUI:

Semi structured data loading tips : 
The schema of semi-structured formats is not strict. That means we don’t know what field names and data types to expect until we read at least part of the file. 
Well, in CSV we may have column names in the first row, but this is not enough in most cases
That’s why each time you open a JSON/CSV/XML dataset in Spark using the simplest API, you wait some time and see jobs executed in WebUI:
# JSON
spark.read.json("sample/json/")

# CSV
spark.read.csv("sample/csv/", header=True, inferSchema=True)

# XML
spark.read.format("com.databricks.spark.xml") \
    .options(rowTag="book").load("sample/xml/")

Hint #1: play with samplingRatio
If your datasets have mostly static schema, there is no need to read all the data. You can speed up loading files with samplingRatio option for JSON and XML readers - the value is from range (0,1] and specifies what fraction of data will be loaded by scheme inferring job.	
# JSON
spark.read.options(samplingRatio=0.1).json("sample/json/")

# XML
spark.read.format("com.databricks.spark.xml") \
    .options(rowTag="book") \
    .options(samplingRatio=0.1) \
    .load("sample/xml/")

# similiar option for CSV does not exist :-(

Hint #2: define static schema
What if your code relies on the schema and schema in the files changes instead define static schema
# JSON
df = spark.read.json("sample/json/", schema=schema)

# CSV
df = spark.read.csv("sample/csv/", schema=schema, header=True)

# XML
df = spark.read.format("com.databricks.spark.xml") \
       .options(rowTag="post").load("sample/xml/", schema=schema)

How to load XML and covert it JSON in spark ?
val df = sqlContext
          .read
          .format("com.databricks.spark.xml")
          .option("rowTag", "OrderSale")
          .load("~/transactions_xml_folder/")

df.printSchema

val jsons = df.toJSON
jsons.saveAsTextFile("~/json_folder/") 	
	
	
