MapReduce framework allows to  process large volume of data-sets in-parallel on large clusters of commodity hardware in a reliable, fault-tolerant manner.

InputSplit
----------
File(HDFS)-->InputFileFormat
             |
			 | --> InputSplits
			 |  
			 | --> InputSplits
			 
			 
InputSplit is the logical representation of data block which is processed by an individual Mapper. 
InputFormat creates the Inputsplit and divide into records, Hence, the mapper process each record (which is a key-value pair)
InputFormat, by default, breaks a file into 128MB chunks (same as blocks in HDFS) and we can also set InputSplit size using mapred.min.split.size parameter in mapred-site.xml 
Inputsplit does not contain the input data, it is just a reference to the data
The number of map tasks is equal to the number of InputSplits

RecordReader : 
---------------
The RecordReader load’s inputSplits data converts into key-value pairs suitable for reading by the mapper.
InputFormat defines the RecordReader, which is responsible for reading actual records from the input files

Map Only Job :
-------------
let us consider a scenario where we just need to perform the operation and no aggregation required, in such case, we will prefer ‘Map-Only job’
We can achieve this by setting job.setNumreduceTasks(0) in the configuration in a driver. 
This will make a number of reducer as 0 and thus the only mapper will be doing the complete task

Reducer:
----------
Reducer takes as input a set of an intermediate key-value pair produced by the mapper and runs a reducer function on them to generate output that is again zero or more key-value pairs

RecordWriter:
-------------
RecordWriter writes Reducer output key-value pairs to output files.

Binary File Format:
------------
A binary file is computer-readable but not human-readable format
A binary format is a format in which file information is stored in the form of ones and zeros

InputFormat:
------------
InputFormat describes the Input-Specification of the job
How the input files are split up and read from HDFS is defined by the InputFormat.
InputFormat defines the InputSplits, which is responsible to split up the files
InputFormat defines the RecordReader, which is responsible for reading actual records from the input files

TextInputFormat:
It is the default InputFormat in MapReduce. 
TextInputFormat treats each line of input file as a separate record 
    Key – It is the byte offset of the beginning of the line
    Value – It is the contents of the line (excluding line terminators.)
This is useful for unformatted data or line-based records like log files.	

KeyValueTextInputFormat:
It also treats each line of input file as a separate record. 
While TextInputFormat treats entire line as the value, but the KeyValueTextInputFormat breaks the line itself into key and value by a tab character (‘/t’). 
Here Key is everything up to the tab character while the value is the remaining part of the line after tab character.

SequenceFileInputFormat:
It reads sequence files. Sequence files are binary files that stores sequences of binary key-value pairs. 
Sequence files are block-compressed and provide direct serialization and deserialization of several arbitrary data types (not just text). 
Here Key & Value both are user-defined.

SequenceFileAsTextInputFormat:
Another form of SequenceFileInputFormat which converts the sequence file key values to Text objects.

SequenceFileAsBinaryInputFormat:
Another form of SequenceFileInputFormat which we can extract the sequence file’s keys and values as an binary object

NLineInputFormat
Another form of TextInputFormat where the keys are byte offset of the line and values are contents of the line. 
Each mapper receives a variable number of lines of input with TextInputFormat and KeyValueTextInputFormat and the number depends on the size of the split and the length of the lines.
And if we want our mapper to receive a fixed number of lines of input, then we use NLineInputFormat.

N is the number of lines of input that each mapper receives. By default (N=1), each mapper receives exactly one line of input. 
If N=2, then each split contains two lines. One mapper will receive the first two Key-Value pairs and another mapper will receive the second two key-value pairs.

DBInputFormat:
It reads data from a relational database, using JDBC. 
Here Key is LongWritables while Value is DBWritables.

Output Format:
--------------
Output Format describes the Output-Specification of the job. 
It determines how RecordWriter implementation is used to write reducer output key-value pairs to output files.

TextOutputFormat :
TextOutputFormat is default Output Format.
It writes (key, value) pairs on individual lines of text files 
Each key-value pair is separated by a tab character, which can be changed using MapReduce.output.textoutputformat.separator property

SequenceFileOutputFormat:
SequenceFileOutputFormat writes keys and values to sequence file as output
Note : SequenceFileInputFormat will de-serialize this sequence file with same type and used for another mapper

SequenceFileAsBinaryOutputFormat: 
It is another form of SequenceFileInputFormat which writes keys and values to sequence file in binary format.

DBOutputFormat: 
Writes to relational databases and HBase

MapFileOutputFormat:
Writes to map files. The key in a MapFile must be added in order

MultipleOutputs: 
It allows writing data to files whose names are derived from the output keys and values, 

LazyOutputFormat: 
Sometimes FileOutputFormat will create output files, even if they are empty. 
LazyOutputFormat writes output file and will be created only when the record is present 


Counters
---------
counters used to collect statistics from a job
In MapReduce Framework, whenever any MapReduce job gets executed, the MapReduce Framework initiates counters to keep track of the job statistics like
Number of mapper and reducer launched..
The number of bytes was read and written
The number of tasks was launched and successfully ran
The amount of CPU and memory consumed is appropriate or not for our job and cluster nodes

We have build in counters and we can also defined customer counters
Built-In counters :
Mapreduce Task Counters
File system counters
Job Counters
Custom Counters : 
Custom Counters are defined by a Java enum. The name of the enum is the group name, and the enum’s fields are the counter names.

Ex : To demonstrate the use of counters we will create counter to count number of
PULMONARY_DISEASE and
BRONCHITIS_ASTHMA

Find the counters in Mapper and increase the counter as
  if(dignosis.contains("PULMONARY") && state.contains("CA")){ context.getCounter(DIAGNOSIS_COUNTER.PULMONARY_DISEASE).increment(1);
 
}
 
if(dignosis.contains(" BRONCHITIS") && state.contains("CA") ){ context.getCounter(DIAGNOSIS_COUNTER.BRONCHITIS_ASTHMA).increment(1);
}

Print : // get all the job related counters
 Counters cn=job.getCounters();
 // Find the specific counters that you want to print
 Counter c1=cn.findCounter(DIAGNOSIS_COUNTER.PULMONARY_DISEASE);
 System.out.println(c1.getDisplayName()+":"+c1.getValue());


Another example custom counters used to determine the number of BAD records
Note : In Job Tracker UI tool we can find the status and statistics of all jobs. 
Using the job tracker UI, developers can view the Counters that have been created and build in counters

Combiners
---------
Combiners also known as Mini-reducers

when we run MapReduce job On a large dataset, Mapper generates large chunks of intermediate data. 
Then this intermediate data will pass to the Reducer for further processing, which leads to massive network congestion

Hadoop framework provides a function known as Combiner that plays a key role in reducing network congestion
The primary job of Combiner is to process the intermediate data from the Mapper, before passing it to Reducer

Combiner will decreases the amount of data that reducer has to process and improves the performance of the reducer.

Speculative Execution : 
-------------------------
MapReduce breaks jobs into tasks and these tasks run parallel.
when a task is running slower than expected time due to like hardware problem or network congestion, Hadoop does not fix slow-running tasks. 
Instead, it will launch an equivalent task as a backup (the backup task is called as speculative task). This process is called speculative execution

The speculative task is killed if the original task completes before the speculative task,
on the other hand, the original task is killed if the speculative task finishes before it

Speculative execution is a MapReduce job optimization technique in Hadoop that is enabled by default. 
You can disable speculative execution for mappers and reducers in mapred-site.xml 
mapred.map.tasks.speculative.execution as false and mapred.reduce.tasks.speculative.execution as false

Data Locality 
------------
Data locality is the process of moving the computation logic close to where the actual data resides on the node, instead of moving large data to computation.
This minimizes network congestion and increases the overall throughput of the system

Hadoop Optimization | Job Optimization & Performance Tuning
-----------------------------------------------------------
LZO compression usage : Enable LZO compression for Map output intermediate data.This decreases IO time during the shuffle
In order to enable LZO compression set mapred.compress.map.output to true
-By Proper tuning of the number of MapReduce tasks
-By using Combiner between mapper and reduce
-By using most appropriate and compact writable type for data
-Reuse the Writables
-Proper configuration of our cluster


Types of Mappers
---------------
Identity Mapper : Default mapper : Identity Mapper class implements the identity function, which directly writes all its input key/value pairs into output

Inverse Mapper :This is a generic mapper class which simply reverses (or swaps) its input (key , value ) pairs into (value, key) pairs in output

Token Counter Mapper :This mapper class, tokenizes its input data (splits data into words) and writes each word with a count of 1 in (word, 1) key-value format

Regex Mapper :This mapper class extracts text matching with the given regular expression.

Chain Mapper
Chain Mapper class can be used to run multiple mappers in a single map task. All mapper classes are run in chained pattern that, the output of the first mapper becomes the input of the second mapper, and so on until the last Mapper, the output of the last Mapper will be written to the task’s output.

No need to specify the output key/value classes for the ChainMapper, this is done by the addMapper() method for the last mapper in the chain

Typef of Reducers
-----------------------
Identity Reducer Default reducer :
It is the default reducer class provided by Hadoop and this class will be picked up by mapreduce job automatically when no other reducer class is specified in the driver class. Similar to Identity Mapper, this class also doesn’t perform any processing on the data and it simply writes all its input data into output.

IntSum Reducer
This reducer class outputs the sum of integer values associated with each reducer input key. 

LongSum Reducer
This reducer class outputs the sum of long values per reducer input key. 

Chain Reducer
Chain Reducer class permits to run a chain of mapper classes after a reducer class within reduce task. The output of the reducer becomes the input of the first mapper and output of the first mapper becomes the input of the second mapper, and so on until the last Mapper, the output of the last Mapper will be written to the task’s output


public class WordCount {
public static void main(String [] args) throws Exception
{
Configuration c=new Configuration();
String[] files=new GenericOptionsParser(c,args).getRemainingArgs();
Path input=new Path(files[0]);
Path output=new Path(files[1]);
Job j=new Job(c,"wordcount");
j.setJarByClass(WordCount.class);
j.setMapperClass(MapForWordCount.class);
j.setReducerClass(ReduceForWordCount.class);
j.setOutputKeyClass(Text.class);
j.setOutputValueClass(IntWritable.class);
FileInputFormat.addInputPath(j, input);
FileOutputFormat.setOutputPath(j, output);
System.exit(j.waitForCompletion(true)?0:1);
}
public static class MapForWordCount extends Mapper<LongWritable, Text, Text, IntWritable>{
public void map(LongWritable key, Text value, Context con) throws IOException, InterruptedException
{
String line = value.toString();
String[] words=line.split(",");
for(String word: words )
{
      Text outputKey = new Text(word.toUpperCase().trim());
  IntWritable outputValue = new IntWritable(1);
  con.write(outputKey, outputValue);
}
}
}
public static class ReduceForWordCount extends Reducer<Text, IntWritable, Text, IntWritable>
{
public void reduce(Text word, Iterable<IntWritable> values, Context con) throws IOException, InterruptedException
{
int sum = 0;
   for(IntWritable value : values)
   {
   sum += value.get();
   }
   con.write(word, new IntWritable(sum));
}
}
}

How to execute MapReduce programs
hadoop jar jarfilename.jar packageName.ClassName  PathToInputTextFile PathToOutputDirectry)

[training@localhost ~]$ hadoop jar MRProgramsDemo.jar PackageDemo.WordCount wordCountFile MRDir1

SplitSize vs BlockSize 
--------------------
Split is a logical division of the input data while block is a physical division of data
So when we store the data, the data will be split and stored based on block size. Default is 128MB
By default SplitSize is same to BlockSize
However we can specify SplitSize to control number of mappers in the processing 
Based on SplitSize number of mappers will be created 











